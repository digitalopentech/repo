version: '1.0'
environment: 
  ENV: prd
  ENV_UPPER: PRD
  ACCOUNT: '81212'
  REGION: sa-east-1
  TOPIC_URL: kafka://broker:9092
  INSTANCE: instance_name
  REFERENCE_DATE: '{{ ds }}'

jobs:
  convert_segments:
    type: dede_spark
    spark_conf:
      spark.sql.sources.partitionOverwriteMode: dynamic
      spark.databricks.delta.schema.autoMerge.enabled: true
      spark.databricks.delta.optimizeWrite.enabled: true
      spark.sql.session.timeZone: UTC
    steps:
      - step_id: convert_segments_parquet
        class: ConvertSegmentsParquet
        jar_path: /path/to/jar
        parameters:
          TABLE: table_name
          S3_PATH: s3://bucket/path
          OUTPUT_DIR: /output/path

tasks:
  create_table:
    type: dede_python
    script: create_table.py
    parameters:
      TABLE: table_name
      KAFKA_REQUIRED_FIELDS: ['field1', 'field2']
      MODULE: module_name
      DOMAIN: domain_name
      CUSTOM_SCHEMA_MODEL_FILE: /path/to/schema/model
      CUSTOM_TABLE_CONFIG_FILE: /path/to/table/config
      KAFKA_SCHEMREGISTRY_TOPIC: kafka_topic
